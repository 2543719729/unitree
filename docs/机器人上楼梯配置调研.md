*图 1：Unitree G1 双足机器人（29 自由度）配备有 3D LiDAR
和深度相机，可实现全景环境感知[\[1\]](https://www.roboworks.net/store/p/unitree-g1-humanoid-robot#:~:text=,depth%20camera%20for%20panoramic%20scanning)。该机器人可在室内环境中通过自主导航完成上下楼梯等复杂运动任务。本文将结合
NVIDIA Isaac Lab 仿真训练和实际机器人部署的经验，详述 G1
机器人自主爬楼的配置与实现流程。内容涵盖仿真环境搭建、传感与控制配置、楼梯识别算法、运动规划与控制策略，以及从仿真迁移到实物部署的要点、所需工具链和常见问题对策等。*

## 1. ISAAC Lab 仿真环境配置与训练

要在 NVIDIA Isaac Lab 中训练 G1
机器人爬楼策略，需要首先搭建逼真的仿真环境，包括**机器人模型**、**楼梯场景**、**传感与控制模块**以及**强化学习任务与奖励函数**等配置。

-   **机器人模型导入**：确保在仿真中加载 Unitree G1
    机器人完整URDF/模型（含29个自由度和末端执行器）。Isaac Lab 已内置对
    G1 机器人的支持，可以直接使用预配置的 G1
    模型[\[2\]](https://isaac-sim.github.io/IsaacLab/main/source/overview/environments.html#:~:text=Image%3A%20velocity)。模型应包含传感器插件，以模拟机载的深度相机和 LiDAR。[\[3\]](https://www.roboworks.net/store/p/unitree-g1-humanoid-robot#:~:text=G1%3A%20Edu%2B)[\[4\]](https://www.roboworks.net/store/p/unitree-g1-humanoid-robot#:~:text=%2A%20Base%20Computing%20Power%3A%208,high%20performance%20CPU)

-   **楼梯环境建模**：构建一个模拟楼梯的室内场景供训练使用。可在 Isaac
    Sim
    中直接创建楼梯几何（例如一系列矩形体代表台阶），或导入预制的楼梯模型。需要设置楼梯的尺寸（踏面宽度、阶高）以匹配现实场景，并考虑在场景中加入墙壁、地板等上下文。Isaac Lab
    框架支持自定义任务场景，通过在`common_scene`目录添加楼梯实体并注册新任务来实现[\[5\]](https://github.com/unitreerobotics/unitree_sim_isaaclab#:~:text=match%20at%20L592%20,scenes%20other%20than%20the%20robot)[\[6\]](https://github.com/unitreerobotics/unitree_sim_isaaclab#:~:text=match%20at%20L603%20,Add%20and%20Register%20Tasks)。配置时应注意楼梯表面摩擦系数和碰撞属性，确保机器人足部接触稳定。

-   **传感器仿真配置**：开启并配置仿真中的**深度相机**和**激光雷达**传感器，以提供与实物相似的感知数据。在
    Isaac Lab
    中可通过命令行参数使能摄像头数据输出（例如`--enable_cameras`）[\[7\]](https://github.com/unitreerobotics/xr_teleoperate#:~:text=%28base%29%20unitree%40Host%3A,robot_type%20g129)。对于 LiDAR，Isaac Sim
    支持添加GPU光线传感器或使用射线投射替代。在强化学习训练中，为简化计算可采用射线高度扫描模拟 LiDAR
    感知[\[8\]](https://forums.developer.nvidia.com/t/deploying-rl-stair-climbing-policy-from-isaac-lab-to-isaac-sim-without-raycast-support/334012#:~:text=I%20trained%20a%20quadruped%20robot,works%20well%20within%20Isaac%20Lab)。例如，一些训练方案让机器人发射多条简化"探测射线"感知前方高度变化，以加速学习[\[8\]](https://forums.developer.nvidia.com/t/deploying-rl-stair-climbing-policy-from-isaac-lab-to-isaac-sim-without-raycast-support/334012#:~:text=I%20trained%20a%20quadruped%20robot,works%20well%20within%20Isaac%20Lab)。在仿真中应尽可能加入传感器噪声和时延，以逼近真实传感器特性。

-   **控制接口配置**：选择机器人在仿真的控制模式和更新频率。可采用关节位置控制（结合PD控制器）或关节力/力矩控制。许多强化学习环境倾向于使用低级PD控制，使策略输出期望关节角度，物理引擎再计算关节力矩，保证稳定性。应将仿真步长和控制频率设置与实物一致（如每秒控制周期与机器人实际控制频率相匹配），以减少策略迁移误差。必要时调低仿真时间步长以确保数值稳定，比如使用子步进模拟快速接触动力学。

-   **强化学习任务定义**：在 Isaac Lab
    中编写楼梯爬升任务的环境类，定义状态观察、动作空间、终止条件和奖励函数等[\[9\]](https://github.com/unitreerobotics/unitree_sim_isaaclab#:~:text=,scenes%20other%20than%20the%20robot)。**观察空间**应包括机器人本体状态（关节角度、速度，IMU姿态等）以及来自传感器的楼梯环境信息（例如前方高度地图、点云片段或射线距离数组）。例如，可将前方一定范围内沿机器人各方向的高度扫描值作为观测[\[8\]](https://forums.developer.nvidia.com/t/deploying-rl-stair-climbing-policy-from-isaac-lab-to-isaac-sim-without-raycast-support/334012#:~:text=I%20trained%20a%20quadruped%20robot,works%20well%20within%20Isaac%20Lab)；若直接使用深度图，则需通过卷积神经网络提取特征。**动作空间**可设为各关节增量位置/速度，或更高级别的腿端轨迹参数。**奖励函数**设计需鼓励机器人**上楼梯的进展**和**稳定性**：通常包括向上/前方移动的距离增益、每登上一级台阶的奖励，以及对摔倒、碰撞的惩罚[\[10\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=The%20secret%20to%20Cassie%E2%80%99s%20stair,speed%20tweaked%2C%20and%20even%20the)。例如，可对机器人质心向上的位移给正奖励，对身体倾斜过大或足部碰撞楼梯棱角给予负奖励，终止条件为机器人跌倒或达到楼梯顶端平台。**奖励
    shaping**非常重要，需要平衡鼓励快速登楼和保持平衡。在训练中逐步提高楼梯难度也有助于收敛------如先用较矮的台阶训练，再逐步增加台阶高度（**课程学习**）。

-   **并行训练与算法**：Isaac Lab 支持并行仿真环境加速训练。可使用
    Stable Baselines3 提供的算法（如 PPO、SAC）直接在 Isaac Sim
    环境上训练[\[11\]](https://forums.developer.nvidia.com/t/isaac-sim-reinforcement-learning/272159#:~:text=Hi%20there%2C%20there%20are%20multiple,of%20the%20Stable%20Baselines3%20library)。由于
    Stable Baselines3 不支持并行多环境，复杂任务常使用 NVIDIA
    提供的 rl-games（OmniIsaacGymEnvs
    框架）来实现并行训练[\[11\]](https://forums.developer.nvidia.com/t/isaac-sim-reinforcement-learning/272159#:~:text=Hi%20there%2C%20there%20are%20multiple,of%20the%20Stable%20Baselines3%20library)。在配置中选定适当的算法和超参数（例如
    PPO 的步长、批量、探索噪声等）。如果使用 Unitree 官方的
    unitree_rl_gym 库，则其内部集成了 ETH Zurich Legged Robotics
    的 RSL-RL PPO
    算法[\[12\]](https://blog.csdn.net/qq_28912651/article/details/149192814#:~:text=1,%E7%89%88%EF%BC%89)。确保策略网络有足够容量（如包含
    LSTM
    记忆单元应对部分可观察环境）和合理的输入归一化，必要时对观测添加高斯噪声增强鲁棒性。

以上配置完成后，可在 Isaac Lab 中启动仿真训练任务。例如，通过运行
`sim_main.py` 并指定自定义楼梯任务和
G1 机器人模型，即可开始强化学习训练过程[\[13\]](https://github.com/unitreerobotics/unitree_sim_isaaclab#:~:text=)。训练过程中监控奖励曲线和机器人的爬楼行为，适当调整奖励权重和传感器设置，直到策略在仿真中能够稳定上楼梯而不跌倒。

## 2. 楼梯自主导航与感知算法配置

实现机器人自主上楼梯，需要**环境感知**算法来识别楼梯并引导机器人导航至楼梯位置并正确攀爬。G1 机器人配备的
3D LiDAR
和深度相机提供了丰富的信息，可以通过融合**点云处理**和**视觉深度分析**实现楼梯检测与导航。主要感知配置如下：

-   **地图构建与定位（SLAM）**：机器人首先需要在室内环境中进行定位和地图构建，以自主规划路径到达楼梯位置。可使用视觉-惯性
    SLAM 算法（例如 Isaac ROS Visual
    SLAM）结合深度相机+IMU数据，实时估计机器人位姿并生成稠密点云地图[\[14\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=1,stack%2C%20which%20Unitree%20also%20supports)。然后利用
    NVIDIA NVBlox 等体素地图工具将深度图/点云转换为局部 ESDF
    地图和2D 栅格成本地图[\[15\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=%28Depth%2FIMU%29%20%E2%86%92%20feed%20into%20,stack%2C%20which%20Unitree%20also%20supports)[\[16\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=1,camera%2Fcolor%2Fcamera_info)。**Nav2**
    等导航框架可利用该成本地图规划机器人从起点到楼梯入口的路径[\[15\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=%28Depth%2FIMU%29%20%E2%86%92%20feed%20into%20,stack%2C%20which%20Unitree%20also%20supports)。在实现中，需要将
    G1 相机和IMU发布为
    ROS 话题（如`/camera/depth/image_raw`，`/imu/data`），并对接
    Isaac ROS VSLAM 与 NVBlox
    节点获取里程计和地图[\[17\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=For%20Unitree%20G1%20Sensor%20Integration%2C,on%20the%20G1%E2%80%99s%20onboard%20computer)。通过这种视觉
    SLAM 与 LiDAR
    地图融合，可得到楼梯周边环境和楼梯本身的精确模型，供路径规划和步态规划使用。

-   **楼梯检测（基于点云）**： *图
    2：深度相机获取的楼梯点云（彩色编码表示深度），算法检测出每级台阶的边缘和高度尺寸[\[18\]](https://index.ros.org/r/stair_step_detector/#:~:text=Image%3A%20stair,are%20displayed%20on%20the%20left)。通过相机标定后，可测得楼梯各级踏板的3D坐标，用于机器人足部落点规划。图示来自开源
    stair-step-detector 框架示例。* 利用 LiDAR
    获取的三维点云，可识别楼梯的结构特征[\[19\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=information%20provided%20by%20depth%20cameras,25)。常用方法是**多平面提取和聚类**：首先对点云进行下采样和平滑，并应用**统计离群点滤波**清除噪点[\[20\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=cloud%20detection%20method%20for%20staircases,of%20LiDAR%20to%20realize%20the)。接着，使用例如
    RANSAC
    的平面分割算法在点云中提取多个近似水平的平面，这些平面候选对应于楼梯的踏面。通过检查平面高度差和排列关系来判断是否构成楼梯：典型室内楼梯由一系列等间距上升的平面组成，平面高度差约为固定值（如每级台阶高
    h），平面前后沿基本平行对齐。检测算法会寻找**多重平行平面**且高度递增的模式，将其识别为楼梯[\[21\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=mainly%20map%20and%20dimension%20staircases,determined%20the%20candidate%20region)[\[22\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=match%20at%20L662%20accurate%20point,improve%20the%20LiDAR%20mapping%20and)。此外，可结合垂直方向的点云投影，寻找点云中的垂直立面（楼梯立板）重复出现的结构，以确认楼梯存在[\[23\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=An%20Environment%20Recognition%20Algorithm%20for,we%20preprocess%20the%20staircase)。一旦识别出楼梯平面序列，算法即可计算楼梯的关键参数：每级踏板高度、深度、宽度及阶数。[\[18\]](https://index.ros.org/r/stair_step_detector/#:~:text=Image%3A%20stair,are%20displayed%20on%20the%20left)指出，通过对深度相机进行地面标定，系统能够测量楼梯各台阶的高度和拐角坐标，实现定量建模。这些信息将用于规划足端落点和步态。例如，若测得台阶高 h=0.2 m、深 d=0.3 m，则机器人需要抬脚至少 0.2 m 以上并向前跨 0.3 m 才可登上一级台阶。

-   **楼梯检测（基于视觉）**：除了点云外，深度相机的图像和深度图也可用于楼梯识别。**图像处理**方法包括检测楼梯在二维图像中的消失点和平行边缘：楼梯台阶边缘在图像中形成平行线条，可通过
    Hough
    变换检测倾斜的直线组，并通过透视几何判断这些线的汇聚点（消失点）来推断楼梯的方向和梯级数量。结合深度图，可以将图像中检测的梯级边缘投影为3D线段并估计每级的位置和尺寸[\[24\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=match%20at%20L620%20information%20provided,25)。Delmerico
    等研究利用深度图分割出水平面并拟合楼梯参数模型（高度、宽度、倾角），取得了对楼梯尺寸\<sup\>\[*\]\</sup\>高精度估计[\[24\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=match%20at%20L620%20information%20provided,25)。另一种方法是机器学习：训练一个卷积神经网络或检测器（如
    YOLOv4-Tiny）识别图像中的楼梯区域[\[25\]](https://www.sciencedirect.com/science/article/abs/pii/S0921889023001744#:~:text=We%20develop%20the%20double%20scales,plane%20joint%20recognition%20%28)。该网络输出楼梯的位置和朝向，再由深度数据确定楼梯的3D定位和梯级参数。这种基于学习的方法对光照和材质变化更鲁棒，可减少误检，但需要大量楼梯图像数据训练。实际实现中，常将传统几何与深度学习结合：例如，先用轻量级CNN检测可能的楼梯区域，在该区域内对点云做精细平面提取，以提升准确率并获取精确尺寸[\[26\]](https://arxiv.org/abs/2211.00610#:~:text=robots%20to%20robustly%20operate%20in,speed%20at%20which%20detections%20occur)。需要注意避免误检*\*：例如墙上类似梯形的花纹、或地上的阴影可能被误认作楼梯。因此算法应结合多传感器信息交叉验证------如 LiDAR
    点云未显示阶梯特征，则即便视觉检测出"楼梯"也需降低置信度。利用 IMU
    数据也可辅助判断：当机器人真正攀爬楼梯时，加速度计/陀螺仪会感测到有节律的抬升和倾斜变化模式；反之在平地不会出现这种模式，可作为楼梯存在与否的佐证。

-   **导航与定位楼梯**：一旦检测到楼梯并地图中标记其位置，机器人需要自主导航至楼梯正前方合适的起始位置。典型做法是将楼梯底部区域作为导航目标点，并在靠近时切换为特殊的楼梯攀爬模式。ROS Nav2
    等规划器通常在2D栅格地图上规划路径，楼梯在2D地图上表现为障碍（因为垂直高度变化）。为使机器人"走上去"，可采用**分层导航**策略：在平地部分由Nav2规划，到达楼梯入口处停止；然后启用专门的楼梯攀爬控制。也有研究尝试在3D地图上直接规划路径通过楼梯（即将楼梯看作可通行空间而非障碍），这需要3D路径搜索算法和非平面运动模型支持，难度较高。在本方案中，更稳妥的是**分段控制**：先自主移动到楼梯前约一阶台阶距离的位置并对准楼梯，然后由楼梯爬升算法接管。[\[27\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=%E2%86%92%20feed%20it%2C%20along%20with,stack%2C%20which%20Unitree%20also%20supports)提到，可利用 NVBlox
    生成局部地图并结合 Nav2 进行本体导航，该过程已包含在 Unitree ROS
    集成中。开发者需确保楼梯入口点的选取合理，并在人形机器人开始上楼前，让其姿态对正楼梯（可通过检测楼梯边缘偏角来调整机身朝向）。如果楼梯有转角平台或中途拐弯，可能需要在平台处重新进行一次环境感知和路径规划，再继续攀登剩余楼梯。

-   **感知数据融合**：在整个过程中，要融合多源传感器数据以可靠感知楼梯。深度相机提供密集的前向视野深度图，但受限于视场和光照；3D LiDAR
    提供全向稀疏点云，能探测到相机视野外的障碍。将两者结合能优势互补：例如在室内光照复杂或有玻璃台阶时，深度摄像头可能失效，但 LiDAR
    仍有效；反之在一些 LiDAR
    死角区域（如很陡的斜面）深度摄像头可直接测量。可以使用**Kalman滤波**或**点云配准**等技术，将相机和 LiDAR
    的感知结果在机器人坐标系中对齐。ROS 框架下，可通过消息同步与 tf
    坐标变换，将相机点云和 LiDAR
    点云合并，再统一进行楼梯平面提取。也可以分别检测后再融合高层结果（如视觉检测到楼梯方向、LiDAR平面给楼梯高度，两者结合完善信息）。关键是在软硬件上确保传感器时间同步和外参标定精确。实际部署时需提前标定深度相机的内参和相对于机器人底座的外参，并标定 LiDAR
    与相机坐标系偏差，以保证感知计算出的楼梯位置准确映射到机器人行动坐标中[\[28\]](https://index.ros.org/r/stair_step_detector/#:~:text=Image%3A%20calibration%20The%20camera%20is,more%20information%20about%20calibration%20here)。

通过以上感知模块配置，机器人能够**自主识别楼梯**、**评估楼梯尺寸**并**规划接近路径**。感知算法会在机器人运动中持续运行：在攀爬过程中，深度相机实时观测前方台阶，修正下一步的落脚位置；LiDAR
则监测周围环境，避免碰撞周边物体（如楼梯扶手或天花板）。这些感知数据为运动规划提供基础，使机器人"看明白"楼梯，从而为安全攀登做好准备。

## 3. 楼梯爬升运动规划与控制策略

实现稳定的楼梯攀爬需要精细的**运动规划**和**控制策略**。常用的方法主要有两类：**基于模型的确定性规划（如预设步态轨迹+状态机）**和**基于学习的策略（强化学习控制）**。实际系统中往往将两者结合，既利用状态机管理步态节奏，又让学习策略执行复杂协调。下面分别介绍相关方案：

-   **基于状态机的步态规划**：这种方法对机器人每一步动作进行明确规划，通过**有限状态机**（FSM）控制双足交替运动。主要状态包括"双足支撑""左脚摆动""右脚摆动"等循环。实现楼梯上升时，FSM
    流程例如：初始双脚在地面支撑→抬起一只脚（摆动腿）→该脚迈上一级台阶并触地稳定→将重心转移到抬脚的一侧（双足短暂支撑）→抬起另一只脚迈上下一台阶→重复循环。每循环完成一阶楼梯的登踏。为保证稳定，需在**重心转移**阶段让机器人身体前倾并将投影落在支撑脚的支撑多边形内（避免失衡）。控制上常采用 ZMP（零力矩点）理论：规划质心轨迹使
    ZMP
    保持在支撑脚区域[\[29\]](https://www.researchgate.net/profile/Mohamed_Mourad_Lafifi/post/How_can_I_get_the_equation_of_start_and_end_hip_position_to_determine_hip_trajectory_while_biped_climbing_stairs/attachment/59d642d179197b807799e69f/AS:440246493028354@1481974351779/download/Motion+Planning+of+Biped+Robot+Climbing+Stairs.pdf#:~:text=ZMP%20must%20be%20kept%20inside,to%20avoid%20obstacles%20and%20to)。在机器人单脚支撑阶段，将机器人动力学等效为倒立摆模型，轨迹规划需保持摆动不使机器人倾倒。[\[30\]](https://www.researchgate.net/profile/Mohamed_Mourad_Lafifi/post/How_can_I_get_the_equation_of_start_and_end_hip_position_to_determine_hip_trajectory_while_biped_climbing_stairs/attachment/59d642d179197b807799e69f/AS:440246493028354@1481974351779/download/Motion+Planning+of+Biped+Robot+Climbing+Stairs.pdf#:~:text=on%20the%20ground%20in%20single,112)提出一种典型方法：通过**三次多项式插值**生成机器人髋部（质心）和摆动脚的轨迹，确保起止状态符合边界条件并满足
    ZMP
    稳定约束[\[30\]](https://www.researchgate.net/profile/Mohamed_Mourad_Lafifi/post/How_can_I_get_the_equation_of_start_and_end_hip_position_to_determine_hip_trajectory_while_biped_climbing_stairs/attachment/59d642d179197b807799e69f/AS:440246493028354@1481974351779/download/Motion+Planning+of+Biped+Robot+Climbing+Stairs.pdf#:~:text=on%20the%20ground%20in%20single,112)。摆动脚轨迹通常设定为**抬脚-向前跨越-下落**的形状，可采用多项式或余弦曲线，使其**足尖避开台阶棱角**并**平稳着陆**[\[31\]](https://www.researchgate.net/profile/Mohamed_Mourad_Lafifi/post/How_can_I_get_the_equation_of_start_and_end_hip_position_to_determine_hip_trajectory_while_biped_climbing_stairs/attachment/59d642d179197b807799e69f/AS:440246493028354@1481974351779/download/Motion+Planning+of+Biped+Robot+Climbing+Stairs.pdf#:~:text=used%20to%20generate%20the%20hip,4)。例如，可在规划中加入一个中间抬脚高度点（高于台阶高度5\~10 cm）确保不会踢到台阶。**图形化轨迹**如余弦或摆线可用于实现自然的抬腿动作。落脚时需要满足**着陆姿态约束**：脚底基本平行于踏面，落地点略微在台阶中心之前以保证踏稳。为实现这些，通常给定每一步的目标落脚位姿（相对于机器人身体坐标系），然后通过**逆运动学**（IK）求解各关节角度命令。与此同时，**上身姿态控制**需要配合：登台阶时机器人上身前倾（类似人类前倾保持平衡），上半身和手臂可作为配重调节以稳定重心。许多先进机器人在爬楼时会微调上肢以平衡，例如当左脚迈上台阶时右臂前摆以平衡惯性。简单实现上可在控制器中加一个身体俯仰角度反馈，用 IMU 测得的倾角控制踝关节或腰部电机输出力矩抵消倾倒趋势（相当于一个**倾斜校正PID**）。

```{=html}
<!-- -->
```
-   实现确定性步态规划需预先知道楼梯参数（每级高度/深度）。这可由前述感知模块提供。控制器读取楼梯高度h和深度d后，可参数化一步跨步长度和高度。例如设置横向步幅
    = d，垂直抬腿高度 = h +
    安全余量。此外，还应考虑楼梯倾斜角对身体姿态的影响，在每登一级后调整机器人身体俯仰角向楼梯倾斜。FSM 架构下，可以设定每一阶段的持续时间（例如单脚摆动0.8秒，双脚支撑0.2秒的步态节奏），并在阶段转换时检查传感器反馈（如新支撑脚是否稳固接触，IMU是否稳定），如不达标则延长支撑相直至稳定。整个规划类似于人形机器人ASIMO、Atlas等使用的模式：预先算好每步足端轨迹和质心轨迹，再通过低层控制跟踪这些轨迹，从而实现流畅爬楼。

    然而，纯预设规划的缺点是缺乏**环境适应性**。如果楼梯高度有偏差或脚底打滑，预设轨迹可能不再最优。为此可以在状态机框架内加入**在线反馈调节**：例如在摆动脚着地瞬间读取足部力传感器，若接触瞬间冲击大则表明落地略晚，可在下一步稍微提前落地时间；或者通过IMU判断身体倾斜，若过度倾斜则加快下一步支撑建立等。这样形成一个**闭环控制**的步态生成。经典的方法如利用预览控制器根据当前重心状态实时调整质心轨迹，或者通过 Capture
    Point（捕获点）算法计算稳定落脚位置。总体而言，基于模型的策略强调**可解释性和安全性**，在已知楼梯情况下可靠性高，但需要繁琐的参数调试，遇到非标准楼梯或随机干扰时适应性有限。

```{=html}
<!-- -->
```
-   **基于强化学习的控制策略**：另一种方法是训练一个端到端的**强化学习策略**让机器人自行学会爬楼梯[\[10\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=The%20secret%20to%20Cassie%E2%80%99s%20stair,speed%20tweaked%2C%20and%20even%20the)。RL 策略由神经网络实现，输入机器人状态和传感器信息，输出关节控制指令。相比精确规划，RL 策略可以**自适应**处理复杂情况，如台阶稍有变化、摩擦不确定等。正如前文所述，我们可以在
    Isaac Lab
    仿真中训练这样一个策略。训练好的策略模型部署到机器人后，由策略网络每 Δt 秒计算下一步动作。在实际执行中，RL 策略表现出与人类类似的行为，而不是严格按照预定轨迹：例如在上楼时，如果脚尖意外碰到台阶，策略可能通过加大抬腿高度来纠正；若机器人身体有点倾斜，策略会快速调整两腿步幅来稳住。这种**反馈式**控制是策略在大量训练中自主学到的鲁棒性技能。

```{=html}
<!-- -->
```
-   一个典型的成功案例是 Cassie
    双足机器人在无视觉输入下通过RL学会了盲走楼梯[\[32\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=You%E2%80%99d%20think%20that%20the%20solution,And%20it%20works%20spectacularly%20well)[\[10\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=The%20secret%20to%20Cassie%E2%80%99s%20stair,speed%20tweaked%2C%20and%20even%20the)。研究者让模拟中的 Cassie
    面对各种不同尺寸的楼梯训练，并在训练时加入关节扰动、随机摩擦等，最终策略实现了无需摄像头，仅靠腿部触觉反馈也能上楼[\[33\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=reinforcement%20learning%20to%20train%20a,speed%20tweaked%2C%20and%20even%20the)。真实测试中，Cassie
    虽然动作看似笨拙，会偶尔**踢到扶手、踩不准台阶边缘**，但依然能够持续登楼而不摔倒[\[34\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=What%20really%20bakes%20my%20noodle,where%20it%20needs%20to%20be)。这体现出RL策略的**容错性**：不追求每一步都完美无误，而是学会了如何在"不完美"的动作后及时调整并继续保持平衡[\[34\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=What%20really%20bakes%20my%20noodle,where%20it%20needs%20to%20be)。相比之下，传统规划往往要求精确落脚，一旦踩偏就难以纠正。RL策略则像小孩学步一样，可以"边笨拙前进边及时纠错"。

    要让RL策略发挥作用，部署时需要注意**观测-行动映射**的合理性。策略通常周期性地感知当前状态并输出控制量，例如每 0.02 s 根据
    IMU姿态、关节角度和前方台阶高度信息输出各关节目标角度或力矩。这样高频闭环控制使策略能在每步过程中微调动作。值得一提的是，虽然RL策略可以端到端控制，但在实际工程中常给它加上一层简化：比如仍由状态机决定哪条腿该抬，作为额外输入给策略网络；或者将策略输出理解为对预设轨迹的修正量而非完全自由动作。这种**混合控制**保留了学习策略的灵活，同时确保基本步态节奏不丢失。

    RL策略的优势在于能够自动探索复杂协调动作（如微妙的重心转移和双腿配合），但也有局限：训练得到的策略有时难以约束，其行为不可解释，可能存在一些训练环境特有的"技巧"[\[35\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=,space%20of%20possible%20actions%2C%20limiting)。因此，在实际部署前，往往需要通过模拟测试各种极端情况验证策略鲁棒性，并可能对策略进行**安全过滤**（如限制关节速度峰值，添加紧急停止逻辑）。总体而言，强化学习策略为楼梯爬行提供了一种**数据驱动**的解决方案，可在复杂和未知楼梯条件下表现出比纯手工规划更强的适应能力。

```{=html}
<!-- -->
```
-   **综合策略**：在实际项目中，可以结合上述方法的优点。例如，使用FSM管理宏观步态序列（何时抬左脚/右脚），而抬脚过程中由RL策略控制具体关节运动，从而兼顾节奏和灵活性。另外，还可以引入**足端轨迹规划 +
    扰动反馈**的混合控制：即先按几何计算一个大致脚步轨迹，再由学习策略或反馈控制在执行过程中实时修正，确保踏准台阶且身体平衡。很多先进的人形机器人控制采用这种**层次控制**结构：高层基于模型规划期望运动，低层基于优化或学习进行追踪和修正。

无论采取何种策略，都必须注重**平衡控制**和**安全冗余**。爬楼过程中机器人重心较高、单脚支撑时间长，容易失稳，必须配合
IMU 和足力传感器进行平衡控制。例如在单脚阶段，可以用腰部关节和摆动腿产生一个摆动力矩抵消倾斜（这相当于人类站单脚时伸展双臂平衡）。若机器人配有力矩控制能力，可实时调节支撑腿的膝关节刚度：当检测到身体前倾，增加膝关节刚度并微伸腿施加反力；如果后倾则相反。此外，若机器人具备灵巧手或其他附着工具，也可用于辅助------比如用手扶墙/扶手稳定自身（G1
有双臂和手，可在必要时抓住栏杆，但这超出了本方案范围）。

总之，楼梯爬升的运动控制需要**精细的时空协调**：空间上要保证足端精确落在台阶上，时间上要协调重心移动与抬腿节奏。模型驱动的方法提供了可控的轨迹，学习驱动的方法提供了鲁棒的适应性。在设计时应根据实际需求取长补短，并通过大量仿真与实际实验调优参数，使机器人既"会走"又"走得稳"。

## 4. 从仿真到真实机器人部署的迁移

将经过 Isaac Lab 仿真训练的楼梯爬升策略成功部署到真实的
Unitree G1 机器人上，需要解决一系列**仿真-现实差距（sim2real
gap）**问题和工程细节。以下从传感同步、控制频率、模型部署和参数调校等方面说明仿真向现实迁移的要点：

-   **传感器数据对齐**：确保真实机器人传感器输出与仿真环境中策略所用的观测具有一致的格式和含义。这涉及时间同步、坐标系一致和数据预处理。例如，如果强化学习策略在仿真中使用的是前方射线高度数组作为输入[\[8\]](https://forums.developer.nvidia.com/t/deploying-rl-stair-climbing-policy-from-isaac-lab-to-isaac-sim-without-raycast-support/334012#:~:text=I%20trained%20a%20quadruped%20robot,works%20well%20within%20Isaac%20Lab)，则在实际机器人上需要编写相应模块，将 LiDAR 点云处理成类似的高度探测结果喂给策略网络。这可能需要选取与仿真射线相同角度的一组 LiDAR 点，或者实时计算机器人前方高度图。要注意**坐标变换**：仿真中高度可能相对于机器人质心，而实际点云通常在世界坐标，需要转换到机器人坐标系并减去机器人当前高度等。类似地，如果策略用了摄像头图像，需要确保相机内参和畸变在现实中已校正，并将图像归一化后输入网络。同样IMU数据（姿态、角速度）也要滤波和零偏校正，使其稳定可靠。建议在机器人静止时采集一组传感器数据，与仿真静态时的数据进行对比，确保统计特性（均值、方差）相近，否则考虑在策略输入层进行归一化处理或调整预处理算法。

-   **控制频率与时序**：仿真中策略可能每 `Δt=20ms` 迭代一次决策，则在实物上也应尽量实现相同的控制周期。例如
    G1
    机器人底层控制周期可能是 200-500 Hz，需要在上层代码中以相应频率调用策略网络，并通过 Unitree
    SDK 发送关节指令。若实际通信或计算无法达到训练频率（例如受限于嵌入式计算性能），可考虑两种对策：一是降低策略频率但每次输出的命令通过插值保持平滑执行；二是对策略进行微调训练以适应新的频率（这可能代价较高）。时序同步也很关键------要避免传感器数据滞后过多。如果传感-\>决策-\>执行总延迟超过一个控制周期，会导致反馈滞后不稳定。可通过在代码中使用**实时线程**或锁住
    CPU 内核来减少时延，并充分利用
    G1 自带的高性能计算单元（据规格内置 100 Tops 算力）运行策略模型[\[36\]](https://www.roboworks.net/store/p/unitree-g1-humanoid-robot#:~:text=,of%20computing%20power)。另外注意，仿真中没有通信延迟，但实际通过网络/总线发送命令存在毫秒级延时且有抖动。策略需要对这点具备鲁棒性，或在策略输出前加预测补偿。例如可在仿真训练时模拟 10 ms 的观测延迟，让策略习惯于旧信息决策，以免现实延迟导致控制滞后。

-   **策略模型部署**：将强化学习得到的神经网络策略加载到机器人实际运行环境中。通常做法是在
    G1 机器人搭载的工控机（或外部控制PC）上运行一个
    ROS2 节点或独立程序，负责读取传感数据-\>经过策略网络计算-\>输出关节命令，通过
    SDK 接口发送到机器人。策略网络一般由 PyTorch 或 TensorFlow
    实现，可直接在Python中加载 `.pt`
    模型并推理。但出于实时性考虑，建议对模型进行**优化**：例如使用
    TensorRT
    将模型转为GPU上高效推理的引擎，或至少启用 TorchScript/ONNX 加速推理。如果
    G1 的计算单元具有 GPU（很可能有，例如 NVIDIA Orin），充分利用GPU算力可大幅提高推理速度，确保不会因计算过慢错过控制周期。在部署过程中应测试每步推理耗时，若超过预算，可考虑减少模型层数或用更快的卷积实现。必要时，可将关键决策部分转为C++实现。Unitree官方的
    SDK 提供 C++/Python API，可灵活选择。本方案假设使用 Python
    部署，借助 `unitree_sdk2_python` 库发送控制命令[\[37\]](https://github.com/unitreerobotics/xr_teleoperate#:~:text=,e)。这个SDK内部通过DDS或UDP协议与机器人实时通信，并封装了低级关节控制指令[\[38\]](https://github.com/unitreerobotics/xr_teleoperate#:~:text=,command%20should%20be%20executed%20on)。开发者需使用SDK的接口，例如 `send_joint_command(position[])` 或 `send_motor_cmd(motor_id, torque)`
    等，将策略输出的关节目标发送出去。请确保通信线程以高优先级运行，并正确处理 SDK 初始化（电机上电等）。

-   **实际控制信号适配**：真实机器人电机控制与仿真模型可能存在差异，需要对策略输出进行一定**适配和限幅**。例如，仿真中可能用无单位的归一化动作输出，而实际需要具体的关节角度或力矩值。要根据仿真模型的关节顺序和零位，将策略输出映射到实际关节指令（这一过程必须使用与训练环境一致的关节映射，否则动作将错乱[\[39\]](https://blog.sssn.tech/?p=856#:~:text=2)）。此外，为保护硬件，应该对输出的关节角/力矩加安全限制：如关节角超出物理极限则截断，力矩过大则限流等。可以在策略输出后增加一个"保险层"，比如一个 safety_check() 函数，校正异常值。当机器人实际行动时，也需监测各关节电流和温度，如发现某关节持续过载（电流超限）或异常发烫，需暂停或降低动作强度。这些措施可防止
    RL
    策略在现实中尝试不切实际的动作导致硬件损坏------仿真里敢跳，在现实里可能需要约束。

-   **参数调优**：现实部署少不了反复测试和参数调整。**摩擦系数差异**是常见问题：仿真中地面-足端摩擦可能与实际楼梯表面不同。若发现机器人实物爬楼时脚下打滑或抓地力不够，可在仿真中调高摩擦重新训练，或通过降低策略期望速度来适应真实摩擦。此外，**关节动力学差异**（比如电机存在静摩擦、关节柔性）在仿真中可能简化了。这个差异表现为策略在实物上动作不如仿真顺畅，甚至发生振荡。解决办法包括：在仿真训练时引入关节模型噪声，或在部署时在控制信号上加**低通滤波**以抑制高频震荡。如果某些关节振动剧烈，可适当调低该关节控制增益。**重心误差**：实物机器人的质量分布、关节零点和仿真或CAD模型可能有细微不同，导致平衡控制偏差。可以通过实际测量机器人重心位置并在仿真模型中校正，或者在策略输入中引入偏置。例如实测发现机器人站立时略向前倾，则可在IMU读数上做补偿平移。**校准**工作还包括传感器的校准：例如对深度相机距离偏差做线性矫正，使其测距与真实标定板一致；对IMU做静态零偏校正等。这些校准能有效提升策略感知的准确性，让策略在现实中基于正确的数据做出决策。

-   **仿真到现实的验证**：建议在正式上楼前，先在**简单场景**中验证策略行为。例如在地面放置一个单阶台阶（如木板）测试机器人迈上/迈下能力。如果使用强化学习策略，可以逐步增加难度：先在平地走，接着迈一个台阶，再两级台阶，最终整个楼梯。这样渐进测试可以发现问题并有针对性调整。在测试过程中，让人员在旁做好保护（必要时挂安全绳，防止摔倒损坏机器人）。观察机器人动作是否与仿真预期相符，特别关注**平衡性**和**足部落点精度**。如果发现机器人走偏了楼梯中心，可能是视觉检测楼梯角度有误或策略没能校正，需要改进感知或引入纠偏机制（比如在上楼过程加入视觉伺服，时时调整朝向）。

-   **策略微调**：若条件允许，可考虑**在真实机器人上微调训练**（Residual Learning）。例如采用已有策略为起点，在现实中使用少量试验数据对策略进行策略梯度更新，以弥补仿真和现实差异。这要求有安全保障措施，比如使用人来遥控保险或限定策略动作幅度。微调能让策略适应真实动力学，但风险较高，需要丰富经验。相对简单的方法是在仿真中提前进行**域随机化**和**参数扰动**[\[33\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=reinforcement%20learning%20to%20train%20a,speed%20tweaked%2C%20and%20even%20the)（如随机变化摩擦、质量、延迟等），已经证明可以提高策略的现实鲁棒性，使其免微调即可良好工作。

通过上述步骤，逐步将仿真得到的控制策略成功移植到实物
G1 机器人上。在整个迁移过程中，要秉持**"谨慎渐进"**原则：先确保各模块（感知-\>决策-\>控制）单独工作正常，再集成测试全流程。初期可在人为监控下运行策略，当策略可靠性提高后再完全自主。经过充分调试，机器人将能够像仿真中一样，在真实楼梯上稳定攀登。

## 5. 软件工具链与系统集成

实现
G1 机器人自主上楼的系统，需要完善的**软件工具链**支撑，包括机器人操作系统、中间件通信、控制 SDK
以及各类算法库的集成。下面介绍本方案涉及的关键软件组件和通信框架：

-   **机器人操作系统 (ROS/ROS 2)**：建议采用 ROS 2
    作为系统集成框架。ROS 2 基于 DDS
    通信中间件，具备实时通信和多语言支持，非常适合多传感器、多算法模块协同。Unitree G1
    官方已提供 ROS 包和
    SDK，方便开发者获取传感器数据和发送运动指令[\[17\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=For%20Unitree%20G1%20Sensor%20Integration%2C,on%20the%20G1%E2%80%99s%20onboard%20computer)。特别地，NVIDIA
    Isaac ROS 提供了视觉SLAM（VSLAM）、NVBlox建图等软件包，可无缝融入
    ROS 2 环境[\[14\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=1,stack%2C%20which%20Unitree%20also%20supports)。在架构上，可设计如下ROS2节点：**感知节点**（处理 LiDAR
    点云和相机图像，发布楼梯位置/尺寸消息）、**定位建图节点**（运行 Isaac ROS VSLAM 和 NVBlox，发布里程计和地图）、**策略控制节点**（运行爬楼策略算法或状态机逻辑，订阅感知信息，发布关节控制命令）、**导航节点**（Nav2，负责平地导航到楼梯口）。各节点通过主题/服务通信，实现数据流转。例如
    G1相机驱动节点发布 `/camera/depth/image_raw`，VSLAM节点订阅并发布机器人 `/odom`，NVBlox订阅 `/odom`和 `/depth`发布本地2D
    costmap，Nav2消费costmap规划路径，策略节点根据感知/命令进行运动控制。ROS 2
    强大的调试工具（rviz可视化、ros2 bag数据记录）也有助于开发过程调试楼梯检测和机器人运动情况。

-   **Unitree SDK
    与通信**：Unitree SDK 是和机器人底层控制交互的接口库。最新的 Unitree SDK2 （unitree_sdk2）针对
    G1/H1 等新机器人，提供 C++/Python API
    控制 29 自由度全身。底层通信可能基于 DDS
    或自定义UDP，但开发者无需关心细节，通过SDK函数即可发送命令[\[38\]](https://github.com/unitreerobotics/xr_teleoperate#:~:text=,command%20should%20be%20executed%20on)。通信频率需按照官方推荐（如500 Hz）来配置。SDK 同时提供机器人状态读取（关节角度、速度、电流、IMU、足底传感器等），这些可发布到
    ROS 主题供各模块使用。在ROS2中可以将Unitree SDK封装为一个**驱动节点**：订阅上层的关节速度/位置命令主题，将指令发送给机器人，同时将机器人状态发布出来。这样上层算法无需直接调用SDK函数，仅通过ROS消息交互即可。值得注意的是，由于
    G1
    是高自由度机体，状态消息很大（包含 29 个关节数据），需要DDS保证高吞吐低延迟。好在ROS2默认使用DDS，且
    Unitree SDK2
    本身似乎也是基于DDS实现[\[38\]](https://github.com/unitreerobotics/xr_teleoperate#:~:text=,command%20should%20be%20executed%20on)，因此二者可以高效整合。在网络部署上，可以选择在机器人本体的工控机上运行大部分ROS2节点（减少延迟），只在外部PC上运行监控和高层接口。也可以全部在机器人本体运行，外部通过无线监控。需要确保无线网络足够稳定带宽，因为调试阶段大量Topic可能通过网络发送。

-   **常用算法与库**：

-   *强化学习库*：若采用 Python 训练，可使用 **Stable
    Baselines3**（封装了 PPO、SAC
    等算法）[\[11\]](https://forums.developer.nvidia.com/t/isaac-sim-reinforcement-learning/272159#:~:text=Hi%20there%2C%20there%20are%20multiple,of%20the%20Stable%20Baselines3%20library)来构建训练脚本，或 **RL Games**（NVIDIA
    提供的 rl library，用于 Isaac Gym/Isaac Lab 并行训练）。Unitree
    官方在其 unitree_rl_gym 中使用了 ETH 的 **RSL-RL** 库（Legged Gym
    中也使用，基于 C++ 实现高性能
    PPO）[\[12\]](https://blog.csdn.net/qq_28912651/article/details/149192814#:~:text=1,%E7%89%88%EF%BC%89)。开发者可以根据偏好选择，只要确保和仿真环境接口匹配即可。另一款值得关注的是
    **skrl**，一个支持PyTorch的通用RL库，已与 Isaac Sim
    集成，可方便地切换算法并支持对抗训练[\[40\]](https://isaac-sim.github.io/IsaacLab/main/source/overview/environments.html#:~:text=Note)。训练好模型后，在部署时可能需要用
    PyTorch/C++ 来加载，相关推理库如 LibTorch、ONNX Runtime、TensorRT
    都是常用工具。

-   *感知与点云库*：**PCL (Point Cloud Library)**
    是处理点云的经典C++库，ROS中有现成接口，可用于滤波、分割、法向估计等[\[41\]](https://index.ros.org/r/stair_step_detector/#:~:text=,only%20for%20the%20ROS2%20wrapper)。例如可用
    PCL 的 `VoxelGrid` 滤波下采样点云、`SACSegmentation`
    提取平面、`EuclideanClusterExtraction` 做点云聚类等。**Open3D**
    则是新兴的点云库，Python接口友好，支持快速体素网格、平面检测，适合需要快速开发的场景。**OpenCV**
    则用于图像处理，在楼梯识别中可用它做边缘检测、形态学运算，以及利用 calib3d
    模块做相机标定和深度-彩色配准[\[41\]](https://index.ros.org/r/stair_step_detector/#:~:text=,only%20for%20the%20ROS2%20wrapper)。**Eigen**
    库常用于矩阵计算、变换；**Boost** 则PCL依赖（例如
    Boost算法和容器）[\[41\]](https://index.ros.org/r/stair_step_detector/#:~:text=,only%20for%20the%20ROS2%20wrapper)。此外，如果使用机器学习进行识别，可借助
    **TensorFlow Lite** 或 **NCNN** 等在机器人上部署轻量级模型，比如一个
    YOLOv5s 楼梯检测网络。总之，感知层面需要的库比较多元，根据开发语言选择合适绑定。

-   *导航与运动规划*：ROS2 Nav2
    提供了全套移动机器人导航方案，包括多传感器定位、路径规划（Dijkstra 或 A* 全局规划、DWA 局部规划）等。然而Nav2本身假设机器人在2D平面移动，不支持爬楼。这种情况下
    Nav2
    主要用来规划到楼梯口的位置[\[27\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=%E2%86%92%20feed%20it%2C%20along%20with,stack%2C%20which%20Unitree%20also%20supports)，之后爬楼动作不通过Nav2控制。因此Nav2的*`planner_server`*、*`controller_server`*等在楼梯模式下会被停用。MoveIt*
    *等运动规划库主要针对机械臂，不直接适用于腿足行走。不过可以用 MoveIt
    中的 IK 功能辅助算腿部逆解。如果需要更高级的步态生成算法，可以考虑*
    *TSID（任务空间逆动力学）或 CroC*\*
    等优化库做全身控制，但这些实现复杂，在本方案中未必需要。

-   *仿真与物理引擎*：NVIDIA Isaac Sim 自带 **PhysX**
    物理引擎，可精确模拟多接触动力学。如需自行二次开发仿真，也可借助
    **Bullet**、**DART** 等引擎，但鉴于 Isaac Lab
    已封装好，不建议重复造轮子。**Omniverse** 的 USD Scene
    格式可以方便地对接自定义环境和模型，在调试楼梯模型时可能用到官方
    **Isaac Sim UI** 或 **omni.isaac.python**
    接口来调整楼梯参数。开发仿真任务时，还会遇到 **Newton欧拉积分**
    设置、**仿真稳定性** 等问题，可参考 NVIDIA 官方文档及 Isaac Lab
    提供的环境范例[\[42\]](https://github.com/unitreerobotics/unitree_sim_isaaclab#:~:text=3)。

-   **开源项目和资源**：社区在这一领域已有不少成果，可供参考和复用：

-   Unitree 官方开源了 **unitree_ros** 包，内含 G1 等机器人 URDF
    模型和基础驱动[\[43\]](https://www.guyuehome.com/wap/detail?id=1906698490635939841#:~:text=%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%EF%BC%9A%E5%AE%87%E6%A0%91%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%BC%80%E6%BA%90%E6%95%B4%E7%90%86%E4%BB%A5%E5%8F%8A%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90%E7%8C%9C%E6%B5%8B%20%E5%9F%BA%E4%BA%8ENVIDIA%20Isaac%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%A4%BA%E4%BE%8B%EF%BC%8C%E6%94%AF%E6%8C%81Go2%E3%80%81H1%E3%80%81G1%E7%AD%89%E5%A4%9A%E4%B8%AA%E5%9E%8B%E5%8F%B7%E6%9C%BA%E5%99%A8%E4%BA%BA%E3%80%82%201516,unitree_ros%2C%20ROS%E4%BB%BF%E7%9C%9F%E5%8C%85%EF%BC%8C%E5%86%85%E5%90%AB%E6%89%80%E6%9C%89Unitree%E7%B3%BB%E5%88%97%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84URDF%E6%96%87%E4%BB%B6%EF%BC%8C%E5%B9%B6%E6%8F%90%E4%BE%9B%E8%AF%A6%E7%BB%86%E7%9A%84)。可使用其
    URDF 和运动示例加快开发。

-   **unitree_sim_isaaclab** 仓库提供了基于 Isaac Lab 的 Unitree
    仿真环境，实现了 G1
    机器人在抓取、行走等任务中的模拟[\[44\]](https://github.com/unitreerobotics/unitree_sim_isaaclab#:~:text=1%E3%80%81%20Introduction)[\[13\]](https://github.com/unitreerobotics/unitree_sim_isaaclab#:~:text=)。其中或许包含传感器设置和任务配置范例，值得借鉴。

-   **stair_step_detector**（ROS2包）前文已提及，是一个 LiDAR
    深度相机楼梯检测方案[\[45\]](https://index.ros.org/r/stair_step_detector/#:~:text=stair)。它为
    RealSense L515 深度激光相机定制，使用 PCL 和 OpenCV
    检测楼梯并输出楼梯模型参数，带有 RViz
    可视化[\[18\]](https://index.ros.org/r/stair_step_detector/#:~:text=Image%3A%20stair,are%20displayed%20on%20the%20left)[\[46\]](https://index.ros.org/r/stair_step_detector/#:~:text=If%20you%20are%20familiar%20with,you%20can%20see%20an%20example)。可以尝试使用或改造该包用于
    G1 感知。

-   **机器人学术论文/算法**：如前述 Cassie
    机器人楼梯攀爬[\[10\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=The%20secret%20to%20Cassie%E2%80%99s%20stair,speed%20tweaked%2C%20and%20even%20the)、"盲踩楼梯"
    RL
    方法[\[47\]](https://arxiv.org/abs/2105.08328#:~:text=Learning%20arxiv,like%20terrain%20on%20the)、以及
    LiDAR +IMU
    楼梯识别算法[\[20\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=cloud%20detection%20method%20for%20staircases,of%20LiDAR%20to%20realize%20the)[\[19\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=information%20provided%20by%20depth%20cameras,25)等。这些提供了很好的思路和算法细节。例如
    Cassie
    的论文[\[10\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=The%20secret%20to%20Cassie%E2%80%99s%20stair,speed%20tweaked%2C%20and%20even%20the)介绍了如何进行
    domain randomization
    提高策略鲁棒性，可直接应用于我们的训练流程；Harbin
    工业大学的楼梯点云检测算法[\[20\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=cloud%20detection%20method%20for%20staircases,of%20LiDAR%20to%20realize%20the)[\[19\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=information%20provided%20by%20depth%20cameras,25)提出了融合多线激光角度特征和平面分割的方法，可用来改进当前楼梯检测的精度。

-   **社区讨论和教程**：NVIDIA 官方论坛和 Reddit 上有许多讨论 Isaac Lab
    与 Unitree G1
    的帖子[\[48\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=I%20found%20these%20two%20libraries%3A,based%20on%20Isaac%20Lab%20and)[\[49\]](https://www.reddit.com/r/reinforcementlearning/comments/1jmzyfp/master_thesis_reinforcement_learning_of_humanoid/#:~:text=Master%20thesis%3A%20Reinforcement%20Learning%20of,goal%20is%20early%20sensor)。例如，有用户分享了在
    Isaac Lab 中模拟 G1 和 Go2
    的仓库，提供了起步模板[\[50\]](https://www.reddit.com/r/robotics/comments/1etwetx/help_needed_in_simulating_unitree_g1_in_isaaclab/#:~:text=Reddit%20www,go2%20robot%20in%20Isaac%20Lab)；NVIDIA
    工程师也在论坛上给出了 G1 结合 Isaac
    ROS 导航的思路[\[51\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=However%2C%20you%20can%20achieve%20this,the%20three%20standard%20stages%20below)。这些都可以作为开发过程中的参考资料。此外，Unitree
    官方文档中心也有 G1 的
    SDK 指南和示例[\[52\]](https://support.unitree.com/home/en/G1_developer/quick_start#:~:text=Motor%20calibration%2823,Step1%3A%20Body%20placement)。

综上，软件工具链的搭建需要协调多方面：利用
ROS 2 作为胶水层，将感知、决策、控制各模块通过主题连接；使用 Unitree SDK
保证与硬件通信的实时可靠；借助丰富的开源库实现复杂的算法功能。通过良好的系统架构设计，可以让各部分模块化、解耦，方便调试和性能优化。当整个软件系统搭建完成后，G1
机器人将具备"眼睛"（传感识别）、"大脑"（决策控制）和"筋骨"（驱动执行）的有机协作能力，胜任自主上楼梯这样的综合任务。

## 6. 常见问题与解决方案

在仿真与现实结合的机器人研发中，难免遇到各种问题。针对楼梯爬行任务，以下列举一些常见挑战及应对思路：

-   **仿真与现实差异**：即 Sim2Real 落差。表现之一是策略在仿真中有效但实物上失败。这通常由模型不精确引起，如摩擦系数、关节阻尼、传感器噪声等差异。**解决**：采用域随机化策略在仿真中提前训练鲁棒性，例如随机扰动摩擦系数、关节驱动延迟和传感器噪声[\[33\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=reinforcement%20learning%20to%20train%20a,speed%20tweaked%2C%20and%20even%20the)。Cassie 实验就通过随机改变关节阻尼、CPU时钟等提高策略泛化[\[33\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=reinforcement%20learning%20to%20train%20a,speed%20tweaked%2C%20and%20even%20the)。此外，在部署初期可以通过调节部分物理参数来逼近现实：如观察机器人在平地站立时是否和仿真一样稳定、摇晃幅度类似，若差别大，可微调仿真中的质心高度或关节弹簧参数，再重新训练。另一个差异是**视觉外观**：仿真环境的纹理和光照与现实不同，导致视觉算法失效。应对方法包括使用逼真的渲染和纹理随机化训练视觉模型，或者干脆采用对视觉不敏感的策略（如Cassie那样不用视觉）。若使用视觉，一种稳妥方案是在真实环境采集一些数据对模型做迁移学习调整，使之适应真实感知。

-   **传感器误差与失效**：实际传感器可能出现测量误差、噪声甚至短暂失效。例如 LiDAR
    可能因为阳光直射产生点云缺失，深度相机在反光材质（如玻璃、金属楼梯）上读不出距离。**解决**：首先软硬件层面尽量减小误差------做好相机曝光控制、红外辅助光源、IMU温漂校准等。然后在算法上采用**滤波与鲁棒估计**：对 LiDAR
    点云用统计滤波去除离群点[\[20\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=cloud%20detection%20method%20for%20staircases,of%20LiDAR%20to%20realize%20the)，对深度图用中值滤波填充空洞。如果传感器出现短暂异常（比如深度图一片无效），可以设计策略短暂停顿或减速，并启用备用感知（例如依赖 LiDAR 点云继续粗略检测楼梯）。多传感器融合也能提高鲁棒性：比如当相机距离失效时靠 LiDAR 测距，当 LiDAR 分辨率不足时用相机补细节。**传感器延迟**
    也属于误差的一种，IMU、相机各自频率不同步可导致感知滞后，可通过时间戳同步及 EKF 融合缓解，使机器人获得实时一致的状态估计。

-   **楼梯检测误判**：包括**漏检**（false negative）和**误检**（false
    positive）。漏检指楼梯存在但算法未识别，可能发生在光照极差或传感器视角不足时；误检则指算法将其他物体错认作楼梯，如地上的条纹、墙上的架子。**解决**：对于漏检，应增加检测的**冗余**手段。例如在机器人移动过程中多角度扫描楼梯，如果某帧没检出但换个角度也许能检出，所以可以在靠近楼梯区域时增大相机仰角、左右摆头扫描等，以获取更多视角。另一个方法是使用**先验信息**：如果机器人知道目标在楼上但迟迟未检测到楼梯，可以推断前方很可能有楼梯，进而降低检测阈值尝试识别。对于误检，需要**交叉验证**：一旦检测出楼梯，要验证其合理性。例如检测结果说前方2米有楼梯，但 LiDAR 没有看到对应的水平面，那很可能是误检[\[53\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=accurate%20point%20cloud%20of%20staircases,overcome%20the%20degradation%20problem%20of)。可以设规则：至少 LiDAR
    点云和视觉都支持，才认为是真楼梯。如果只有单一来源，则要求连续多帧稳定检测才确认。此外，还可根据上下文过滤：楼梯通常连接上下两个水平面且靠墙，如检测的"楼梯"悬空或者不连地面，可排除[\[54\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=common%20in%20real%20life%2C%20the,results%20are%20shown%20in%20Table)。误检一旦确定，需要让机器人忽略该虚假目标，并可能调整算法参数（如提高视觉检测置信度门限）。实际中，多模态融合和稳定的识别机制可以将误报率降到很低。

-   **动作执行误差**：包括脚步定位不准和姿态控制误差。可能原因有机械精度误差、地面不平等。表现为脚没踩稳台阶边缘或者身体晃动大。**解决**：可以在闭环中加入**感觉反馈**。例如在脚触地瞬间，通过关节力矩或加速度传感器感知冲击，若落地点偏差则立即进行微调（比如脚还未完全放平就已受力，说明踩在台阶棱角上了，可通过踝关节或脚腕关节立刻调整角度让脚平放）。另外一个方法是**脚尖探测**：人类上楼时往往用脚尖先触碰一下确认台阶位置，机器人也可模仿------在正式落脚前快速向下轻触，如果比预期早碰到说明台阶高了，稍抬再向前；若迟迟碰不到说明可能踏空，需要稍往下延伸或者停止。虽然实现复杂，但这是一种提高落脚精度的冗余手段。姿态控制方面，如果发现机器人上楼时身体后倾明显，可在每迈一步后增加一个短暂的静稳相，等待身体摆正再进行下一步（虽然降低效率，但安全）。也可以调整步态参数，比如**减小步长**、**降低重心**来增稳。特别地，G1 机器人有上肢，可以在必要时通过摆动手臂来修正角动量（如感觉向后倒时举起双臂向前甩以产生反扭矩）。

-   **硬件局限**：楼梯高度和机器人能力可能存在矛盾。如果楼梯台阶特别高（超过机器人腿长的极限步幅）则无论如何也上不去；或者楼梯很窄机器人放不下两脚。这些属于**任务不可行**的情况，需要在任务层面避免。例如限定机器人只爬其能力范围内的楼梯（G1腿长约0.6m，最大阶高估计0.3m左右）。对于较陡楼梯，可以尝试侧身爬（双足横向排列登阶）或者借助手扶，但那超出了常规控制范围。另一个硬件问题是**电池和连续功率**：爬楼是高功率动作，长楼梯可能造成电机过热或电压骤降。解决方法包括监控温度，在过热前暂停休息，以及路径规划上避免连续长楼梯（如可在中途平台休息片刻）。此外，确保电池电量充足以防途中电压不足导致扭矩下降。建议在上楼前检查各关节状态和电量，必要时更换电池或降低载荷。

-   **安全和故障恢复**：机器人爬楼一旦失去平衡将严重跌落损坏，因此需要有安全策略。例如安装机械**保险绳/杆**在调试阶段；软件上，当
    IMU
    检测到姿态超过安全阈值（如倾斜\>45°）且无法恢复时，立即执行**紧急停止**或进入"坐下"姿态，以降低跌落高度伤害。故障恢复方面，如果机器人在楼梯中段不慎停止（比如脚卡住或策略陷入不收敛振荡），需要有人工/自动的恢复方案，例如将其切换到手动遥控模式或触发一个预设的撤离动作（缓慢后退下楼）。这些虽然不直接提升性能，但对于实际部署必不可少。

通过以上针对性措施，可以大大提高机器人楼梯爬行任务在现实中的成功率和安全性。总的来说，应在**仿真阶段尽可能暴露并解决问题**，但仍要做好**现实调试的准备**。小的问题往往可以通过调整参数解决，而大的差异可能需要修改算法甚至策略重训。开发者要有耐心，逐一排查影响因素。例如，先解决感知可靠，再调控足步轨迹，最后优化平衡鲁棒。每遇到一个问题，利用仿真复现该问题场景，修正后再部署验证，这样不断逼近理想性能。

经过充分的调校，当常见问题都得到缓解后，Unitree G1
机器人将能在室内环境中自主识别楼梯、规划路径并稳健地一步一步攀登上去，完成用户设定的跨楼层移动任务。这将极大拓展双足机器人的应用场景和自主能力。通过上述系统性的技术方案，我们为实现这一目标提供了详细指南和可行路径。

**参考文献：**

1.  Unitree Robotics -- *"Unitree G1"*
    产品介绍[\[55\]](https://www.roboworks.net/store/p/unitree-g1-humanoid-robot#:~:text=,kg%2C%20height%20127%20cm)[\[3\]](https://www.roboworks.net/store/p/unitree-g1-humanoid-robot#:~:text=G1%3A%20Edu%2B)

2.  GitHub -- unitreerobotics/*"unitree_sim_isaaclab"* 仓库
    README[\[5\]](https://github.com/unitreerobotics/unitree_sim_isaaclab#:~:text=match%20at%20L592%20,scenes%20other%20than%20the%20robot)[\[6\]](https://github.com/unitreerobotics/unitree_sim_isaaclab#:~:text=match%20at%20L603%20,Add%20and%20Register%20Tasks)

3.  NVIDIA Developer Forums -- *"Deploying RL Stair-Climbing Policy from
    Isaac Lab to Isaac
    Sim"*[\[8\]](https://forums.developer.nvidia.com/t/deploying-rl-stair-climbing-policy-from-isaac-lab-to-isaac-sim-without-raycast-support/334012#:~:text=I%20trained%20a%20quadruped%20robot,works%20well%20within%20Isaac%20Lab)

4.  IEEE Spectrum -- Evan Ackerman, *"Cassie Is Now Astonishingly Good
    at Stairs"*
    (2021)[\[10\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=The%20secret%20to%20Cassie%E2%80%99s%20stair,speed%20tweaked%2C%20and%20even%20the)[\[34\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=What%20really%20bakes%20my%20noodle,where%20it%20needs%20to%20be)

5.  ROS 2 Package -- *"stair_step_detector"* -- Peter Nebe
    (2024)[\[18\]](https://index.ros.org/r/stair_step_detector/#:~:text=Image%3A%20stair,are%20displayed%20on%20the%20left)[\[46\]](https://index.ros.org/r/stair_step_detector/#:~:text=If%20you%20are%20familiar%20with,you%20can%20see%20an%20example)

6.  MDPI Remote Sensing -- Liu et al., *"An Environment Recognition
    Algorithm for Staircase Climbing Robots"*
    (2024)[\[20\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=cloud%20detection%20method%20for%20staircases,of%20LiDAR%20to%20realize%20the)[\[19\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=information%20provided%20by%20depth%20cameras,25)

7.  NVIDIA Developer Forums -- *"Unitree G1 + VSLAM + NVBlox
    Integration"*
    (2025)[\[14\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=1,stack%2C%20which%20Unitree%20also%20supports)[\[17\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=For%20Unitree%20G1%20Sensor%20Integration%2C,on%20the%20G1%E2%80%99s%20onboard%20computer)

8.  NVIDIA Isaac Lab Documentation -- *"Available Environments & Tasks"*
    (2025)[\[2\]](https://isaac-sim.github.io/IsaacLab/main/source/overview/environments.html#:~:text=Image%3A%20velocity)

9.  CSDN博客 --
    *"宇树 G1 部署（四）------强化学习运动控制（IsaacGym版）"*
    (2025)[\[12\]](https://blog.csdn.net/qq_28912651/article/details/149192814#:~:text=1,%E7%89%88%EF%BC%89)[\[56\]](https://blog.csdn.net/qq_28912651/article/details/149192814#:~:text=1)

10. NVIDIA Developer Forums -- *"Isaac Sim Reinforcement Learning
    (Custom RL Example)"*
    (2023)[\[11\]](https://forums.developer.nvidia.com/t/isaac-sim-reinforcement-learning/272159#:~:text=Hi%20there%2C%20there%20are%20multiple,of%20the%20Stable%20Baselines3%20library)

\<small\>*注：以上内容结合了文献资料和实际经验整理，括号内【†】所示为引文出处行号。*\</small\>

[\[1\]](https://www.roboworks.net/store/p/unitree-g1-humanoid-robot#:~:text=,depth%20camera%20for%20panoramic%20scanning)
[\[3\]](https://www.roboworks.net/store/p/unitree-g1-humanoid-robot#:~:text=G1%3A%20Edu%2B)
[\[4\]](https://www.roboworks.net/store/p/unitree-g1-humanoid-robot#:~:text=%2A%20Base%20Computing%20Power%3A%208,high%20performance%20CPU)
[\[36\]](https://www.roboworks.net/store/p/unitree-g1-humanoid-robot#:~:text=,of%20computing%20power)
[\[55\]](https://www.roboworks.net/store/p/unitree-g1-humanoid-robot#:~:text=,kg%2C%20height%20127%20cm)
Unitree G1 \| Roboworks

<https://www.roboworks.net/store/p/unitree-g1-humanoid-robot>

[\[2\]](https://isaac-sim.github.io/IsaacLab/main/source/overview/environments.html#:~:text=Image%3A%20velocity)
[\[40\]](https://isaac-sim.github.io/IsaacLab/main/source/overview/environments.html#:~:text=Note)
Available Environments --- Isaac Lab Documentation

<https://isaac-sim.github.io/IsaacLab/main/source/overview/environments.html>

[\[5\]](https://github.com/unitreerobotics/unitree_sim_isaaclab#:~:text=match%20at%20L592%20,scenes%20other%20than%20the%20robot)
[\[6\]](https://github.com/unitreerobotics/unitree_sim_isaaclab#:~:text=match%20at%20L603%20,Add%20and%20Register%20Tasks)
[\[9\]](https://github.com/unitreerobotics/unitree_sim_isaaclab#:~:text=,scenes%20other%20than%20the%20robot)
[\[13\]](https://github.com/unitreerobotics/unitree_sim_isaaclab#:~:text=)
[\[42\]](https://github.com/unitreerobotics/unitree_sim_isaaclab#:~:text=3)
[\[44\]](https://github.com/unitreerobotics/unitree_sim_isaaclab#:~:text=1%E3%80%81%20Introduction)
GitHub - unitreerobotics/unitree_sim_isaaclab: The Unitree simulation
environment built based on Isaac Lab

<https://github.com/unitreerobotics/unitree_sim_isaaclab>

[\[7\]](https://github.com/unitreerobotics/xr_teleoperate#:~:text=%28base%29%20unitree%40Host%3A,robot_type%20g129)
[\[37\]](https://github.com/unitreerobotics/xr_teleoperate#:~:text=,e)
[\[38\]](https://github.com/unitreerobotics/xr_teleoperate#:~:text=,command%20should%20be%20executed%20on)
GitHub - unitreerobotics/xr_teleoperate: This repository implements
teleoperation of the Unitree humanoid robot using XR Devices.

<https://github.com/unitreerobotics/xr_teleoperate>

[\[8\]](https://forums.developer.nvidia.com/t/deploying-rl-stair-climbing-policy-from-isaac-lab-to-isaac-sim-without-raycast-support/334012#:~:text=I%20trained%20a%20quadruped%20robot,works%20well%20within%20Isaac%20Lab)
Deploying RL Stair-Climbing Policy from Isaac Lab to Isaac Sim Without
Raycast Support - Isaac Sim - NVIDIA Developer Forums

<https://forums.developer.nvidia.com/t/deploying-rl-stair-climbing-policy-from-isaac-lab-to-isaac-sim-without-raycast-support/334012>

[\[10\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=The%20secret%20to%20Cassie%E2%80%99s%20stair,speed%20tweaked%2C%20and%20even%20the)
[\[32\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=You%E2%80%99d%20think%20that%20the%20solution,And%20it%20works%20spectacularly%20well)
[\[33\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=reinforcement%20learning%20to%20train%20a,speed%20tweaked%2C%20and%20even%20the)
[\[34\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=What%20really%20bakes%20my%20noodle,where%20it%20needs%20to%20be)
[\[35\]](https://spectrum.ieee.org/agility-robotics-cassie-stairs#:~:text=,space%20of%20possible%20actions%2C%20limiting)
Agility Robotics\' Cassie Is Now Astonishingly Good at Stairs - IEEE
Spectrum

<https://spectrum.ieee.org/agility-robotics-cassie-stairs>

[\[11\]](https://forums.developer.nvidia.com/t/isaac-sim-reinforcement-learning/272159#:~:text=Hi%20there%2C%20there%20are%20multiple,of%20the%20Stable%20Baselines3%20library)
Isaac sim reinforcement learning - Isaac Sim - NVIDIA Developer Forums

<https://forums.developer.nvidia.com/t/isaac-sim-reinforcement-learning/272159>

[\[12\]](https://blog.csdn.net/qq_28912651/article/details/149192814#:~:text=1,%E7%89%88%EF%BC%89)
[\[56\]](https://blog.csdn.net/qq_28912651/article/details/149192814#:~:text=1)
宇树 G1 部署（四）------强化学习运动控制（IsaacGym 版）\_issac
gym-CSDN博客

<https://blog.csdn.net/qq_28912651/article/details/149192814>

[\[14\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=1,stack%2C%20which%20Unitree%20also%20supports)
[\[15\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=%28Depth%2FIMU%29%20%E2%86%92%20feed%20into%20,stack%2C%20which%20Unitree%20also%20supports)
[\[16\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=1,camera%2Fcolor%2Fcamera_info)
[\[17\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=For%20Unitree%20G1%20Sensor%20Integration%2C,on%20the%20G1%E2%80%99s%20onboard%20computer)
[\[27\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=%E2%86%92%20feed%20it%2C%20along%20with,stack%2C%20which%20Unitree%20also%20supports)
[\[48\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=I%20found%20these%20two%20libraries%3A,based%20on%20Isaac%20Lab%20and)
[\[51\]](https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691#:~:text=However%2C%20you%20can%20achieve%20this,the%20three%20standard%20stages%20below)
Unitree g1+nvblox+vslam - Isaac ROS - NVIDIA Developer Forums

<https://forums.developer.nvidia.com/t/unitree-g1-nvblox-vslam/348691>

[\[18\]](https://index.ros.org/r/stair_step_detector/#:~:text=Image%3A%20stair,are%20displayed%20on%20the%20left)
[\[28\]](https://index.ros.org/r/stair_step_detector/#:~:text=Image%3A%20calibration%20The%20camera%20is,more%20information%20about%20calibration%20here)
[\[41\]](https://index.ros.org/r/stair_step_detector/#:~:text=,only%20for%20the%20ROS2%20wrapper)
[\[45\]](https://index.ros.org/r/stair_step_detector/#:~:text=stair)
[\[46\]](https://index.ros.org/r/stair_step_detector/#:~:text=If%20you%20are%20familiar%20with,you%20can%20see%20an%20example)
stair_step_detector - ROS Repository Overview

<https://index.ros.org/r/stair_step_detector/>

[\[19\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=information%20provided%20by%20depth%20cameras,25)
[\[20\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=cloud%20detection%20method%20for%20staircases,of%20LiDAR%20to%20realize%20the)
[\[21\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=mainly%20map%20and%20dimension%20staircases,determined%20the%20candidate%20region)
[\[22\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=match%20at%20L662%20accurate%20point,improve%20the%20LiDAR%20mapping%20and)
[\[23\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=An%20Environment%20Recognition%20Algorithm%20for,we%20preprocess%20the%20staircase)
[\[24\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=match%20at%20L620%20information%20provided,25)
[\[53\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=accurate%20point%20cloud%20of%20staircases,overcome%20the%20degradation%20problem%20of)
[\[54\]](https://www.mdpi.com/2072-4292/16/24/4718#:~:text=common%20in%20real%20life%2C%20the,results%20are%20shown%20in%20Table)
An Environment Recognition Algorithm for Staircase Climbing Robots

<https://www.mdpi.com/2072-4292/16/24/4718>

[\[25\]](https://www.sciencedirect.com/science/article/abs/pii/S0921889023001744#:~:text=We%20develop%20the%20double%20scales,plane%20joint%20recognition%20%28)
Identifying and approaching for obscured stairs - ScienceDirect.com

<https://www.sciencedirect.com/science/article/abs/pii/S0921889023001744>

[\[26\]](https://arxiv.org/abs/2211.00610#:~:text=robots%20to%20robustly%20operate%20in,speed%20at%20which%20detections%20occur)
\[2211.00610\] Fast Staircase Detection and Estimation using 3D Point
Clouds with Multi-detection Merging for Heterogeneous Robots

<https://arxiv.org/abs/2211.00610>

[\[29\]](https://www.researchgate.net/profile/Mohamed_Mourad_Lafifi/post/How_can_I_get_the_equation_of_start_and_end_hip_position_to_determine_hip_trajectory_while_biped_climbing_stairs/attachment/59d642d179197b807799e69f/AS:440246493028354@1481974351779/download/Motion+Planning+of+Biped+Robot+Climbing+Stairs.pdf#:~:text=ZMP%20must%20be%20kept%20inside,to%20avoid%20obstacles%20and%20to)
[\[30\]](https://www.researchgate.net/profile/Mohamed_Mourad_Lafifi/post/How_can_I_get_the_equation_of_start_and_end_hip_position_to_determine_hip_trajectory_while_biped_climbing_stairs/attachment/59d642d179197b807799e69f/AS:440246493028354@1481974351779/download/Motion+Planning+of+Biped+Robot+Climbing+Stairs.pdf#:~:text=on%20the%20ground%20in%20single,112)
[\[31\]](https://www.researchgate.net/profile/Mohamed_Mourad_Lafifi/post/How_can_I_get_the_equation_of_start_and_end_hip_position_to_determine_hip_trajectory_while_biped_climbing_stairs/attachment/59d642d179197b807799e69f/AS:440246493028354@1481974351779/download/Motion+Planning+of+Biped+Robot+Climbing+Stairs.pdf#:~:text=used%20to%20generate%20the%20hip,4)
zhang_fira.dvi

<https://www.researchgate.net/profile/Mohamed_Mourad_Lafifi/post/How_can_I_get_the_equation_of_start_and_end_hip_position_to_determine_hip_trajectory_while_biped_climbing_stairs/attachment/59d642d179197b807799e69f/AS:440246493028354@1481974351779/download/Motion+Planning+of+Biped+Robot+Climbing+Stairs.pdf>

[\[39\]](https://blog.sssn.tech/?p=856#:~:text=2)
基于Isaaclab的人形机器人仿真与强化学习训练 -- 石上三年Official

<https://blog.sssn.tech/?p=856>

[\[43\]](https://www.guyuehome.com/wap/detail?id=1906698490635939841#:~:text=%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%EF%BC%9A%E5%AE%87%E6%A0%91%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%BC%80%E6%BA%90%E6%95%B4%E7%90%86%E4%BB%A5%E5%8F%8A%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90%E7%8C%9C%E6%B5%8B%20%E5%9F%BA%E4%BA%8ENVIDIA%20Isaac%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%A4%BA%E4%BE%8B%EF%BC%8C%E6%94%AF%E6%8C%81Go2%E3%80%81H1%E3%80%81G1%E7%AD%89%E5%A4%9A%E4%B8%AA%E5%9E%8B%E5%8F%B7%E6%9C%BA%E5%99%A8%E4%BA%BA%E3%80%82%201516,unitree_ros%2C%20ROS%E4%BB%BF%E7%9C%9F%E5%8C%85%EF%BC%8C%E5%86%85%E5%90%AB%E6%89%80%E6%9C%89Unitree%E7%B3%BB%E5%88%97%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84URDF%E6%96%87%E4%BB%B6%EF%BC%8C%E5%B9%B6%E6%8F%90%E4%BE%9B%E8%AF%A6%E7%BB%86%E7%9A%84)
具身智能：宇树机器人开源整理以及技术解析猜测

<https://www.guyuehome.com/wap/detail?id=1906698490635939841>

[\[47\]](https://arxiv.org/abs/2105.08328#:~:text=Learning%20arxiv,like%20terrain%20on%20the)
Blind Bipedal Stair Traversal via Sim-to-Real Reinforcement Learning

<https://arxiv.org/abs/2105.08328>

[\[49\]](https://www.reddit.com/r/reinforcementlearning/comments/1jmzyfp/master_thesis_reinforcement_learning_of_humanoid/#:~:text=Master%20thesis%3A%20Reinforcement%20Learning%20of,goal%20is%20early%20sensor)
Master thesis: Reinforcement Learning of humanoid robot Unitree G1

<https://www.reddit.com/r/reinforcementlearning/comments/1jmzyfp/master_thesis_reinforcement_learning_of_humanoid/>

[\[50\]](https://www.reddit.com/r/robotics/comments/1etwetx/help_needed_in_simulating_unitree_g1_in_isaaclab/#:~:text=Reddit%20www,go2%20robot%20in%20Isaac%20Lab)
Help needed in simulating Unitree G1 in IsaacLab : r/robotics - Reddit

<https://www.reddit.com/r/robotics/comments/1etwetx/help_needed_in_simulating_unitree_g1_in_isaaclab/>

[\[52\]](https://support.unitree.com/home/en/G1_developer/quick_start#:~:text=Motor%20calibration%2823,Step1%3A%20Body%20placement)
G1 SDK Development Guide - 宇树文档中心

<https://support.unitree.com/home/en/G1_developer/quick_start>
